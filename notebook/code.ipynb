{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5427,
     "status": "ok",
     "timestamp": 1747607408337,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "Ga-oXOSubaKr",
    "outputId": "4b125bea-23ff-43aa-94c9-c313695ec570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version:  2.12.0 , GPU: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Proyectos Python\\TG\\venv\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]='2'\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "availale_GPUs = len(physical_devices)\n",
    "print('Using TensorFlow version: ', tf.__version__, ', GPU:', availale_GPUs)\n",
    "if physical_devices:\n",
    "    try:\n",
    "        for gpu in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from tensorflow.keras.layers import Input, Activation, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1747607408608,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "rg7qN1qDdHr4"
   },
   "outputs": [],
   "source": [
    "# Extract data from dataset\n",
    "\n",
    "\n",
    "WS_data = scipy.io.loadmat('..\\\\data\\\\Weather_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20847,
     "status": "ok",
     "timestamp": 1747607429463,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "FPSxJ3EHe4yS",
    "outputId": "a3676097-33ab-4315-912e-537b8508ed4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-check for NaN in time sequence 0\n"
     ]
    }
   ],
   "source": [
    "# Convert date to continuous time: from date and time format to seconds\n",
    "date_0 = WS_data['Date'][0]\n",
    "date = []\n",
    "for i in range(0, len(date_0)):\n",
    "    date = np.append(date, str(date_0[i])[2 : -2])\n",
    "time_init = dt.datetime(int(date[0][0 : 4]), int(date[0][5 : 7]), int(date[0][8 : 10]), int(date[0][11 : 13]), int(date[0][14 : 16]))\n",
    "T_nan_index = np.argwhere(pd.isna(date))\n",
    "date = np.delete(date, T_nan_index[:, 0],  0)\n",
    "print('Double-check for NaN in time sequence', np.sum(pd.isna(date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1747607429580,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "A71pYdO6iHKl"
   },
   "outputs": [],
   "source": [
    "Seconds = np.zeros((date.shape[0], 1))\n",
    "for index in range(date.shape[0]):\n",
    "    Seconds[index, 0] = ((dt.datetime(int(date[index][0 : 4]), int(date[index][5 : 7]), int(date[index][8 : 10]), int(date[index][11 : 13]), int(date[index][14 : 16])) - time_init).total_seconds())\n",
    "T_WS = Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747607429593,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "-t7tEYSehVUX"
   },
   "outputs": [],
   "source": [
    "# Convert to Cartesian coordinates\n",
    "X_WS = np.array(6378000 * np.sin(np.radians(WS_data['Lon'])))[0]  # Longitude to meters\n",
    "Y_WS = np.array(6378000 * np.sin(np.radians(WS_data['Lat'])))[0]  # Latitude to meters\n",
    "Z_WS = np.array(WS_data['Alt'])[0]\n",
    "Temp_WS = np.array(WS_data['Temperature'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747607429605,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "KqbAX-DiipIP"
   },
   "outputs": [],
   "source": [
    "# Project wind speed and direction into Cartesian coordinates\n",
    "U_WS = (WS_data['WindSpeed'] * WS_data['WindDirectionX'])[0]\n",
    "V_WS = (WS_data['WindSpeed'] * WS_data['WindDirectionY'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747607429608,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "dCmCdCOFk345"
   },
   "outputs": [],
   "source": [
    "# Pressure from mbar to Pa\n",
    "P_WS = WS_data['Pressure'][0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747607429621,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "r4JYL4Mak5db"
   },
   "outputs": [],
   "source": [
    "# Remove NaN values from time field\n",
    "X_WS = np.delete(X_WS, T_nan_index[:, 0],  0)\n",
    "Y_WS = np.delete(Y_WS, T_nan_index[:, 0],  0)\n",
    "Z_WS = np.delete(Z_WS, T_nan_index[:, 0],  0)\n",
    "U_WS = np.delete(U_WS, T_nan_index[:, 0],  0)\n",
    "V_WS = np.delete(V_WS, T_nan_index[:, 0],  0)\n",
    "P_WS = np.delete(P_WS, T_nan_index[:, 0],  0)\n",
    "Temp_WS = np.delete(Temp_WS, T_nan_index[:, 0],  0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1747607429685,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "arpzY1xVk7Kg",
    "outputId": "db9ab190-ddc6-4c47-a080-04ecd413feab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weather stations: 21\n"
     ]
    }
   ],
   "source": [
    "# Structure data into matrix: 21 available stations (rows) x measurement every 10 min (column)\n",
    "T_WS = np.reshape(T_WS, (int(T_WS.shape[0] / 21), 21)).T # There are 21 WS stations in this case\n",
    "X_WS = np.reshape(X_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "Y_WS = np.reshape(Y_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "Z_WS = np.reshape(Z_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "U_WS = np.reshape(U_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "V_WS = np.reshape(V_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "P_WS = np.reshape(P_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "Temp_WS = np.reshape(Temp_WS, (T_WS.shape[1], T_WS.shape[0])).T\n",
    "print('Number of weather stations:', T_WS.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1747607429686,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "DS0-9D-Uk9Ck",
    "outputId": "c7dde395-1567-47f4-907f-8834808e63b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-check for NaN in location field 0\n"
     ]
    }
   ],
   "source": [
    "# Remove NaN from location data\n",
    "X_nan_index = np.argwhere(np.isnan(X_WS))\n",
    "T_WS = np.delete(T_WS, X_nan_index[:, 0],  0)\n",
    "P_WS = np.delete(P_WS, X_nan_index[:, 0],  0)\n",
    "U_WS = np.delete(U_WS, X_nan_index[:, 0],  0)\n",
    "V_WS = np.delete(V_WS, X_nan_index[:, 0],  0)\n",
    "X_WS = np.delete(X_WS, X_nan_index[:, 0],  0)\n",
    "Y_WS = np.delete(Y_WS, X_nan_index[:, 0],  0)\n",
    "Z_WS = np.delete(Z_WS, X_nan_index[:, 0],  0)\n",
    "Temp_WS = np.delete(Temp_WS, X_nan_index[:, 0],  0)\n",
    "print('Double-check for NaN in location field', np.sum(np.isnan(X_WS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1747607429686,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "JusIIBFRlBJc"
   },
   "outputs": [],
   "source": [
    "# Days selected for reconstruction\n",
    "n_days = 14 # Change up to a maximum of 14 availsble days\n",
    "samples =  int(144 * n_days) # Convert selected days to snapshots\n",
    "T_WS = T_WS[:, : samples]\n",
    "X_WS = X_WS[:, : samples]\n",
    "Y_WS = Y_WS[:, : samples]\n",
    "Z_WS = Z_WS[:, : samples]\n",
    "U_WS = U_WS[:, : samples]\n",
    "V_WS = V_WS[:, : samples]\n",
    "P_WS = P_WS[:, : samples]\n",
    "Temp_WS = Temp_WS[:, : samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747607429694,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "Z9hA23PwlFZ_"
   },
   "outputs": [],
   "source": [
    "# Sort values in matrix into increasing values of X coordinate\n",
    "for snap in range(0, T_WS.shape[1]):\n",
    "    index_sort = np.argsort(X_WS[:, snap])\n",
    "    T_WS[:, snap] = T_WS[index_sort, snap]\n",
    "    X_WS[:, snap] = X_WS[index_sort, snap]\n",
    "    Y_WS[:, snap] = Y_WS[index_sort, snap]\n",
    "    Z_WS[:, snap] = Z_WS[index_sort, snap]\n",
    "    U_WS[:, snap] = U_WS[index_sort, snap]\n",
    "    V_WS[:, snap] = V_WS[index_sort, snap]\n",
    "    P_WS[:, snap] = P_WS[index_sort, snap]\n",
    "    Temp_WS[:, snap] = Temp_WS[index_sort, snap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747607429711,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "3R7HdeBelHj7"
   },
   "outputs": [],
   "source": [
    "# Delete NaN from U, V and P if constantly occuring for each weather station\n",
    "uvp_mean = np.nanmean(np.concatenate([U_WS, V_WS, P_WS], axis = 1), axis = 1)[:, None]\n",
    "vel_nan_index = np.argwhere(np.isnan(uvp_mean))\n",
    "T_WS = np.delete(T_WS, vel_nan_index[:, 0],  0)\n",
    "P_WS = np.delete(P_WS, vel_nan_index[:, 0],  0)\n",
    "U_WS = np.delete(U_WS, vel_nan_index[:, 0],  0)\n",
    "V_WS = np.delete(V_WS, vel_nan_index[:, 0],  0)\n",
    "X_WS = np.delete(X_WS, vel_nan_index[:, 0],  0)\n",
    "Y_WS = np.delete(Y_WS, vel_nan_index[:, 0],  0)\n",
    "Z_WS = np.delete(Z_WS, vel_nan_index[:, 0],  0)\n",
    "Temp_WS = np.delete(Temp_WS, vel_nan_index[:, 0],  0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747607429712,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "uzijh35TlJOf"
   },
   "outputs": [],
   "source": [
    "# Correct pressure to sea level (ISA)\n",
    "P_WS = P_WS * (1 - 0.0065 * Z_WS / (Temp_WS + 273.15 + 0.0065 * Z_WS))**(-5.257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747607429713,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "pFrtOAEMlLCT"
   },
   "outputs": [],
   "source": [
    "# Certering of location and time fields\n",
    "x_min = np.min(X_WS)\n",
    "x_max = np.max(X_WS)\n",
    "X_WS = X_WS - (x_min + x_max) / 2\n",
    "y_min = np.min(Y_WS)\n",
    "y_max = np.max(Y_WS)\n",
    "Y_WS = Y_WS - (y_min + y_max) / 2\n",
    "t_min = np.min(T_WS)\n",
    "t_max = np.max(T_WS)\n",
    "T_WS = T_WS - t_min # Refer to t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747607429725,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "VtS2XKrqlMwj"
   },
   "outputs": [],
   "source": [
    "# PINN output grid\n",
    "T_PINN = T_WS[0 : 1, :] # Same times for reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747607429725,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "ijwNJ1smlPAI"
   },
   "outputs": [],
   "source": [
    "# Resolution in degrees\n",
    "R = 0.2\n",
    "R_PINN = 6378000 * np.sin(np.radians(R)) # Grid resolution\n",
    "x_PINN = np.arange(x_min - R_PINN, x_max + R_PINN, R_PINN) # X values in output resolution\n",
    "y_PINN = np.arange(y_min - R_PINN, y_max + R_PINN, R_PINN) # Y values in output resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747607429726,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "6JAa9AZ9lRAw"
   },
   "outputs": [],
   "source": [
    "# Centering of location data\n",
    "x_PINN = x_PINN - (x_min + x_max) / 2\n",
    "y_PINN = y_PINN - (y_min + y_max) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747607429780,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "xSlCDKxplTMR"
   },
   "outputs": [],
   "source": [
    "# Final output grid\n",
    "X_PINN, Y_PINN = np.meshgrid(x_PINN, y_PINN)\n",
    "X_PINN = X_PINN.flatten('F')[:, None]\n",
    "Y_PINN = Y_PINN.flatten('F')[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747607429791,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "pV6Miw43lU85"
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "dim_T_PINN = T_PINN.shape[1]\n",
    "dim_N_PINN = X_PINN.shape[0]\n",
    "\n",
    "T_PINN = np.tile(T_PINN, (dim_N_PINN, 1))\n",
    "X_PINN = np.tile(X_PINN, dim_T_PINN)\n",
    "Y_PINN = np.tile(Y_PINN, dim_T_PINN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747607429792,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "bf-ue-Dwla_V",
    "outputId": "84822777-a17e-4d53-9f69-20c8ba7116cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L: 409791.6159110926 W 16.718180196652117 P0 100359.48707895375 Re 495728659759\n"
     ]
    }
   ],
   "source": [
    "# Reference values for non-dimensionalization\n",
    "L = np.sqrt((x_max - x_min) ** 2 + (y_max - y_min) ** 2) # Reference distance\n",
    "W = np.sqrt(np.nanmax(abs(U_WS)) ** 2 + np.nanmax(abs(V_WS)) ** 2) # Reference velocity\n",
    "rho = 1.269 # Air density at 15 degrees\n",
    "nu = 1.382e-5 # Kinematic viscosity at 15 degrees\n",
    "Re = int(W * L / nu) # Reynolds number\n",
    "P0 = np.nanmean(P_WS) # Reference pressure level\n",
    "print('L:', L, 'W', W, 'P0', P0, 'Re', Re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1747607429793,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "IL_1xz60lfBO"
   },
   "outputs": [],
   "source": [
    "# Non-dimensionalization\n",
    "X_WS = X_WS / L\n",
    "Y_WS = Y_WS / L\n",
    "T_WS = T_WS * W / L\n",
    "P_WS = (P_WS - P0) / rho / (W ** 2)\n",
    "U_WS = U_WS / W\n",
    "V_WS = V_WS / W\n",
    "\n",
    "X_PINN = X_PINN / L\n",
    "Y_PINN = Y_PINN / L\n",
    "T_PINN = T_PINN * W / L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747607429793,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "_Xb23ymRlihf"
   },
   "outputs": [],
   "source": [
    "# Validation cases (remove stations)\n",
    "# # N_test = 0 # Number of stations to remove\n",
    "WS_val = np.array([1, 2, 3, 5, 7, 9, 10, 11, 13, 14, 15, 16, 19])\n",
    "# Choose between different arrays for desired validation case:\n",
    "# Close: np.array([2, 8, 10, 14, 16, 19])\n",
    "# Far: np.array([0, 1, 4, 6, 8, 12, 17, 18, 19, 20])\n",
    "# Envelope: np.array([1, 2, 3, 5, 7, 9, 10, 11, 13, 14, 15, 16, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747607429794,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "geJyAoGhlgsi"
   },
   "outputs": [],
   "source": [
    "# Remove WS for validation\n",
    "T_val = T_WS[WS_val, :]\n",
    "P_val = P_WS[WS_val, :]\n",
    "U_val = U_WS[WS_val, :]\n",
    "V_val = V_WS[WS_val, :]\n",
    "X_val = X_WS[WS_val, :]\n",
    "Y_val = Y_WS[WS_val, :]\n",
    "Z_val = Z_WS[WS_val, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1747607429800,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "vWeZ9iUUlkcW",
    "outputId": "431151a3-90cb-4549-a92d-bace232a4dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of final weather stations available for training: 8\n"
     ]
    }
   ],
   "source": [
    "# Remaining Ws for training\n",
    "T_WS = np.delete(T_WS, WS_val, 0)\n",
    "P_WS = np.delete(P_WS, WS_val, 0)\n",
    "U_WS = np.delete(U_WS, WS_val, 0)\n",
    "V_WS = np.delete(V_WS, WS_val, 0)\n",
    "X_WS = np.delete(X_WS, WS_val, 0)\n",
    "Y_WS = np.delete(Y_WS, WS_val, 0)\n",
    "print('Number of final weather stations available for training:', T_WS.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747607429802,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "wJQyrCXelmKK"
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "dim_N_WS = X_WS.shape[0]\n",
    "dim_T_WS = X_WS.shape[1]\n",
    "\n",
    "del WS_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747607429815,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "U1363lN5loOT"
   },
   "outputs": [],
   "source": [
    "# Customized dense layer\n",
    "class GammaBiasLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, *args, **kwargs):\n",
    "        super(GammaBiasLayer, self).__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight('bias',\n",
    "                                    shape=(self.units,),\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.gamma = self.add_weight('gamma',\n",
    "                                     shape = (self.units,),\n",
    "                                     initializer = 'ones',\n",
    "                                     trainable = True)\n",
    "\n",
    "        self.w = tfa.layers.WeightNormalization(Dense(self.units, use_bias = False,\n",
    "                                    kernel_initializer = tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None),\n",
    "                                    trainable = True, activation = None))\n",
    "\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        return self.gamma * self.w(input_tensor) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1551,
     "status": "ok",
     "timestamp": 1747607431368,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "dZpl26l3lqJU",
    "outputId": "13a6f860-f1d1-4f33-c449-1a8836b1c14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3)]               0         \n",
      "                                                                 \n",
      " gamma_bias_layer (GammaBias  (None, 600)              5401      \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " activation (Activation)     (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_1 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_2 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_3 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_4 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_5 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_6 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 600)               0         \n",
      "                                                                 \n",
      " gamma_bias_layer_7 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " gamma_bias_layer_8 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " gamma_bias_layer_9 (GammaBi  (None, 600)              721801    \n",
      " asLayer)                                                        \n",
      "                                                                 \n",
      " gamma_bias_layer_10 (GammaB  (None, 600)              721801    \n",
      " iasLayer)                                                       \n",
      "                                                                 \n",
      " gamma_bias_layer_11 (GammaB  (None, 600)              721801    \n",
      " iasLayer)                                                       \n",
      "                                                                 \n",
      " gamma_bias_layer_12 (GammaB  (None, 3)                3610      \n",
      " iasLayer)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,948,822\n",
      "Trainable params: 3,985,209\n",
      "Non-trainable params: 3,963,613\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "num_input_variables = 3 # t, x, y\n",
    "num_output_variables = 3 # u, v, p\n",
    "\n",
    "neurons = 200 * num_output_variables\n",
    "layers = [num_input_variables] + (2 * (num_input_variables + num_output_variables))*[neurons] + [num_output_variables]\n",
    "\n",
    "inputs = Input(shape = (num_input_variables, ))\n",
    "h = GammaBiasLayer(layers[1])(inputs)\n",
    "h = Activation('tanh')(h)\n",
    "for l in layers[2 : 2 * int((len(layers) - 2) / 3)]:\n",
    "    h = GammaBiasLayer(l)(h)\n",
    "    h = Activation('tanh')(h)\n",
    "for l in layers[2 * int((len(layers) - 2) / 3) : -1]:\n",
    "    h = GammaBiasLayer(layers[-2])(h)\n",
    "outputs = GammaBiasLayer(layers[-1])(h)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747607431372,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "tc4N93WWpvf1"
   },
   "outputs": [],
   "source": [
    "# Error functions and loss function\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "@tf.function(reduce_retracing = True)\n",
    "def loss_NS_2D(model, t_eqns_batch, x_eqns_batch, y_eqns_batch, training):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    with tf.GradientTape(persistent = True) as tape1:\n",
    "        tape1.watch((t_eqns_batch, x_eqns_batch, y_eqns_batch))\n",
    "        X_eqns_batch = tf.concat([t_eqns_batch, x_eqns_batch, y_eqns_batch], axis = 1)\n",
    "        Y_eqns_batch = model(X_eqns_batch, training = training)\n",
    "        [u_eqns_pred, v_eqns_pred, p_eqns_pred] = tf.split(Y_eqns_batch, num_or_size_splits=Y_eqns_batch.shape[1], axis=1)\n",
    "\n",
    "    # Derivatives\n",
    "    u_t_eqns_pred = tape1.gradient(u_eqns_pred, t_eqns_batch)\n",
    "    v_t_eqns_pred = tape1.gradient(v_eqns_pred, t_eqns_batch)\n",
    "\n",
    "    u_x_eqns_pred = tape1.gradient(u_eqns_pred, x_eqns_batch)\n",
    "    v_x_eqns_pred = tape1.gradient(v_eqns_pred, x_eqns_batch)\n",
    "    p_x_eqns_pred = tape1.gradient(p_eqns_pred, x_eqns_batch)\n",
    "\n",
    "    u_y_eqns_pred = tape1.gradient(u_eqns_pred, y_eqns_batch)\n",
    "    v_y_eqns_pred = tape1.gradient(v_eqns_pred, y_eqns_batch)\n",
    "    p_y_eqns_pred = tape1.gradient(p_eqns_pred, y_eqns_batch)\n",
    "\n",
    "    # Navier-Stokes residuals\n",
    "    e1 = (u_x_eqns_pred + v_y_eqns_pred)\n",
    "    e2 = (u_t_eqns_pred + (u_eqns_pred * u_x_eqns_pred + v_eqns_pred * u_y_eqns_pred) + p_x_eqns_pred)\n",
    "    e3 = (v_t_eqns_pred + (u_eqns_pred * v_x_eqns_pred + v_eqns_pred * v_y_eqns_pred) + p_y_eqns_pred)\n",
    "\n",
    "    return mse(0, e1) + mse(0, e2) + mse(0, e3)\n",
    "\n",
    "def loss_u(model, t_data_batch, x_data_batch, y_data_batch, u_data_batch, training):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    X_data_batch = tf.concat([t_data_batch, x_data_batch, y_data_batch], axis = 1)\n",
    "    Y_data_batch = model(X_data_batch, training = training)\n",
    "    [u_data_pred, _, _] = tf.split(Y_data_batch, num_or_size_splits=Y_data_batch.shape[1], axis=1)\n",
    "\n",
    "    return mse(u_data_batch, u_data_pred) / tf.math.reduce_std(u_data_batch)**2\n",
    "\n",
    "def loss_v(model, t_data_batch, x_data_batch, y_data_batch, v_data_batch, training):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    X_data_batch = tf.concat([t_data_batch, x_data_batch, y_data_batch], axis = 1)\n",
    "    Y_data_batch = model(X_data_batch, training = training)\n",
    "    [_, v_data_pred, _] = tf.split(Y_data_batch, num_or_size_splits=Y_data_batch.shape[1], axis=1)\n",
    "\n",
    "    return mse(v_data_batch, v_data_pred) / tf.math.reduce_std(v_data_batch)**2\n",
    "\n",
    "def loss_p(model, t_data_batch, x_data_batch, y_data_batch, p_data_batch, training):\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    X_data_batch = tf.concat([t_data_batch, x_data_batch, y_data_batch], axis = 1)\n",
    "    Y_data_batch = model(X_data_batch, training = training)\n",
    "    [_, _, p_data_pred] = tf.split(Y_data_batch, num_or_size_splits=Y_data_batch.shape[1], axis=1)\n",
    "\n",
    "    return mse(p_data_batch, p_data_pred) / tf.math.reduce_std(p_data_batch)**2\n",
    "\n",
    "def loss_total(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch, t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb, training):\n",
    "    NS_eqns = lamb * loss_NS_2D(model, t_eqns_batch, x_eqns_batch, y_eqns_batch, training)\n",
    "    NS_data = lamb * loss_NS_2D(model, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, training)\n",
    "    P_e = loss_p(model, t_p_batch, x_p_batch, y_p_batch, p_p_batch, training)\n",
    "    U_e = loss_u(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, training)\n",
    "    V_e = loss_v(model, t_v_batch, x_v_batch, y_v_batch, v_v_batch, training)\n",
    "\n",
    "    total_e = NS_eqns + NS_data + U_e + V_e + P_e\n",
    "\n",
    "    return  (NS_eqns ** 2 + NS_data**2 + U_e ** 2 + V_e ** 2 + P_e ** 2) / total_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747607431376,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "6jY1RckVp2fV"
   },
   "outputs": [],
   "source": [
    "# Optimize model - gradients:\n",
    "def grad(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch,  t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_total(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch,  t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb, training = True)\n",
    "    gradient_model = tape.gradient(loss_value, model.trainable_variables)\n",
    "    return loss_value, gradient_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747607431405,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "Bd-SXtTtp5eI"
   },
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "model_optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1747607431408,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "pGUwmAuQp7F-"
   },
   "outputs": [],
   "source": [
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "NS_loss_results = []\n",
    "P_loss_results = []\n",
    "U_loss_results = []\n",
    "V_loss_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747607431409,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "xetSaHmFp9hj"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 5 # number of epochs\n",
    "# num_epochs = 1000 # number of epochs\n",
    "lamb = 2 # Tuning of physics constraints\n",
    "batch_PINN = int(np.ceil((dim_N_PINN * dim_T_PINN / n_days * R)))\n",
    "batch_WS = int(np.ceil(dim_N_WS * dim_T_WS / n_days * R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747607431411,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "_M_V92Kkp_Jk"
   },
   "outputs": [],
   "source": [
    "# Data dimensions\n",
    "dim_N_data = dim_N_WS\n",
    "dim_T_data = dim_T_WS\n",
    "dim_T_eqns = dim_T_PINN\n",
    "dim_N_eqns = dim_N_PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6337251,
     "status": "ok",
     "timestamp": 1747613768645,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "RO7BUA4rqAlH",
    "outputId": "d0cce82a-8df1-4ade-d0cf-13cb83d8eff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss_training: 2.633e+00 NS_loss: 5.764e-01 P_loss: 9.779e-01 U_loss: 1.268e+00 V_loss: 3.319e+00\n",
      "Epoch: 1 Loss_training: 2.308e+00 NS_loss: 3.125e-01 P_loss: 8.022e-01 U_loss: 8.344e-01 V_loss: 2.735e+00\n",
      "Epoch: 2 Loss_training: 2.563e+00 NS_loss: 3.581e-01 P_loss: 5.818e-01 U_loss: 2.086e+00 V_loss: 2.215e+00\n",
      "Epoch: 3 Loss_training: 1.278e+00 NS_loss: 2.063e-01 P_loss: 5.629e-01 U_loss: 1.097e+00 V_loss: 1.148e+00\n",
      "Epoch: 4 Loss_training: 1.329e+00 NS_loss: 2.150e-01 P_loss: 4.375e-01 U_loss: 1.012e+00 V_loss: 1.641e+00\n",
      "Process completed\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_NS_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_P_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_U_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_V_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    # Data mixing and shuffling\n",
    "    idx_t = np.random.choice(dim_T_WS, dim_T_data, replace = False)\n",
    "    idx_x = np.random.choice(dim_N_WS, dim_N_data, replace = False)\n",
    "    t_u = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    x_u = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    y_u = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    z_u = Z_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    u_u = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    v_u = V_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    p_u = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "\n",
    "    idx_t = np.random.choice(dim_T_WS, dim_T_data, replace = False)\n",
    "    idx_x = np.random.choice(dim_N_WS, dim_N_data, replace = False)\n",
    "    t_v = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    x_v = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    y_v = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    z_v = Z_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    u_v = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    v_v = V_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    p_v = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "\n",
    "    idx_t = np.random.choice(P_WS.shape[1], P_WS.shape[1], replace = False)\n",
    "    idx_x = np.random.choice(P_WS.shape[0], P_WS.shape[0], replace = False)\n",
    "    t_p = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    x_p = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    y_p = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    z_p = Z_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    u_p = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    v_p = V_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    p_p = P_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "\n",
    "    idx_t = np.random.choice(dim_T_PINN, dim_T_eqns, replace = False)\n",
    "    idx_x = np.random.choice(dim_N_PINN, dim_N_eqns, replace = False)\n",
    "    t_eqns = T_PINN[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    x_eqns = X_PINN[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    y_eqns = Y_PINN[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "\n",
    "    idx_t = np.random.choice(dim_T_WS, dim_T_data, replace = False)\n",
    "    idx_x = np.random.choice(dim_N_WS, dim_N_data, replace = False)\n",
    "    t_eqns_ref = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    x_eqns_ref = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "    y_eqns_ref = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n",
    "\n",
    "    idx_batch = np.random.choice(t_u.shape[0], t_u.shape[0], replace = False)\n",
    "    t_u = t_u[idx_batch, :]\n",
    "    x_u = x_u[idx_batch, :]\n",
    "    y_u = y_u[idx_batch, :]\n",
    "    u_u = u_u[idx_batch, :]\n",
    "    idx_batch = np.random.choice(t_v.shape[0], t_v.shape[0], replace = False)\n",
    "    t_v = t_v[idx_batch, :]\n",
    "    x_v = x_v[idx_batch, :]\n",
    "    y_v = y_v[idx_batch, :]\n",
    "    v_v = v_v[idx_batch, :]\n",
    "    idx_batch = np.random.choice(t_p.shape[0], t_p.shape[0], replace = False)\n",
    "    t_p = t_p[idx_batch, :]\n",
    "    x_p = x_p[idx_batch, :]\n",
    "    y_p = y_p[idx_batch, :]\n",
    "    p_p = p_p[idx_batch, :]\n",
    "    idx_batch = np.random.choice(t_eqns.shape[0], t_eqns.shape[0], replace = False)\n",
    "    t_eqns = t_eqns[idx_batch, :]\n",
    "    x_eqns = x_eqns[idx_batch, :]\n",
    "    y_eqns = y_eqns[idx_batch, :]\n",
    "    idx_batch = np.random.choice(t_eqns_ref.shape[0], t_eqns_ref.shape[0], replace = False)\n",
    "    t_eqns_ref = t_eqns_ref[idx_batch, :]\n",
    "    x_eqns_ref = x_eqns_ref[idx_batch, :]\n",
    "    y_eqns_ref = y_eqns_ref[idx_batch, :]\n",
    "\n",
    "    # Remove remaining NaN\n",
    "    nan_index = np.argwhere(np.isnan(u_u))\n",
    "    t_u = np.delete(t_u, nan_index[:, 0],  0)\n",
    "    u_u = np.delete(u_u, nan_index[:, 0],  0)\n",
    "    x_u = np.delete(x_u, nan_index[:, 0],  0)\n",
    "    y_u = np.delete(y_u, nan_index[:, 0],  0)\n",
    "    nan_index = np.argwhere(np.isnan(v_v))\n",
    "    t_v = np.delete(t_v, nan_index[:, 0],  0)\n",
    "    v_v = np.delete(v_v, nan_index[:, 0],  0)\n",
    "    x_v = np.delete(x_v, nan_index[:, 0],  0)\n",
    "    y_v = np.delete(y_v, nan_index[:, 0],  0)\n",
    "    nan_index = np.argwhere(np.isnan(p_p))\n",
    "    t_p = np.delete(t_p, nan_index[:, 0],  0)\n",
    "    p_p = np.delete(p_p, nan_index[:, 0],  0)\n",
    "    x_p = np.delete(x_p, nan_index[:, 0],  0)\n",
    "    y_p = np.delete(y_p, nan_index[:, 0],  0)\n",
    "\n",
    "    # Batch size distribution\n",
    "    div_u = range(0, len(x_u), batch_WS)\n",
    "    div_v = range(0, len(x_v), batch_WS)\n",
    "    div_p = range(0, len(x_p), batch_WS)\n",
    "    div_eqns = range(0, len(x_eqns_ref), batch_WS)\n",
    "    div_PINN = range(0, len(x_eqns), batch_PINN)\n",
    "\n",
    "    min_div = min([len(div_u), len(div_v), len(div_p), len(div_eqns), len(div_PINN)])\n",
    "\n",
    "    # Batch step\n",
    "    for index in range(0, min_div):\n",
    "        index_u = div_u[index]\n",
    "        index_v = div_v[index]\n",
    "        index_p = div_p[index]\n",
    "        index_eqns = div_eqns[index]\n",
    "        index_PINN = div_PINN[index]\n",
    "        t_u_batch = tf.convert_to_tensor(t_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n",
    "        x_u_batch = tf.convert_to_tensor(x_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n",
    "        y_u_batch = tf.convert_to_tensor(y_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n",
    "        u_u_batch = tf.convert_to_tensor(u_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n",
    "        v_u_batch = tf.convert_to_tensor(v_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n",
    "        t_v_batch = tf.convert_to_tensor(t_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n",
    "        x_v_batch = tf.convert_to_tensor(x_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n",
    "        y_v_batch = tf.convert_to_tensor(y_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n",
    "        u_v_batch = tf.convert_to_tensor(u_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n",
    "        v_v_batch = tf.convert_to_tensor(v_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n",
    "        t_p_batch = tf.convert_to_tensor(t_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n",
    "        x_p_batch = tf.convert_to_tensor(x_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n",
    "        y_p_batch = tf.convert_to_tensor(y_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n",
    "        u_p_batch = tf.convert_to_tensor(u_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n",
    "        v_p_batch = tf.convert_to_tensor(v_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n",
    "        p_p_batch = tf.convert_to_tensor(p_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n",
    "        t_eqns_ref_batch = tf.convert_to_tensor(t_eqns_ref[index_eqns : index_eqns + batch_WS, :], dtype = 'float32')\n",
    "        x_eqns_ref_batch = tf.convert_to_tensor(x_eqns_ref[index_eqns : index_eqns + batch_WS, :], dtype = 'float32')\n",
    "        y_eqns_ref_batch = tf.convert_to_tensor(y_eqns_ref[index_eqns : index_eqns + batch_WS, :], dtype = 'float32')\n",
    "        t_eqns_batch = tf.convert_to_tensor(t_eqns[index_PINN : index_PINN + batch_PINN, :], dtype = 'float32')\n",
    "        x_eqns_batch = tf.convert_to_tensor(x_eqns[index_PINN : index_PINN + batch_PINN, :], dtype = 'float32')\n",
    "        y_eqns_batch = tf.convert_to_tensor(y_eqns[index_PINN : index_PINN + batch_PINN, :], dtype = 'float32')\n",
    "\n",
    "        loss_train, grads = grad(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch,  t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb)\n",
    "\n",
    "        NS_loss = loss_NS_2D(model, t_eqns_batch, x_eqns_batch, y_eqns_batch, training = False)\n",
    "        P_loss = loss_p(model, t_p_batch, x_p_batch, y_p_batch, p_p_batch, training = False)\n",
    "        U_loss = loss_u(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, training = False)\n",
    "        V_loss = loss_v(model, t_v_batch, x_v_batch, y_v_batch, v_v_batch, training = False)\n",
    "\n",
    "        model_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        epoch_loss_avg.update_state(loss_train)\n",
    "        epoch_NS_loss_avg.update_state(NS_loss)\n",
    "        epoch_P_loss_avg.update_state(P_loss)\n",
    "        epoch_U_loss_avg.update_state(U_loss)\n",
    "        epoch_V_loss_avg.update_state(V_loss)\n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    NS_loss_results.append(epoch_NS_loss_avg.result())\n",
    "    P_loss_results.append(epoch_P_loss_avg.result())\n",
    "    U_loss_results.append(epoch_U_loss_avg.result())\n",
    "    V_loss_results.append(epoch_V_loss_avg.result())\n",
    "\n",
    "    # Update learning rate (adaptive)\n",
    "    if epoch_loss_avg.result() > 1e-1:\n",
    "        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    elif epoch_loss_avg.result() > 3e-2:\n",
    "        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    elif epoch_loss_avg.result() > 3e-3:\n",
    "        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    else:\n",
    "        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "    print(\"Epoch: {:d} Loss_training: {:.3e} NS_loss: {:.3e} P_loss: {:.3e} U_loss: {:.3e} V_loss: {:.3e}\".format(epoch, epoch_loss_avg.result(),\n",
    "    epoch_NS_loss_avg.result(), epoch_P_loss_avg.result(), epoch_U_loss_avg.result(), epoch_V_loss_avg.result()))\n",
    "\n",
    "    ################# Save Data ###########################\n",
    "    if (epoch + 1) % num_epochs == 0:\n",
    "        # Output in higher resolution\n",
    "        U_PINN = np.zeros_like(X_PINN)\n",
    "        V_PINN = np.zeros_like(X_PINN)\n",
    "        P_PINN = np.zeros_like(X_PINN)\n",
    "        # Values predicted on WS locations\n",
    "        U_WS_pred = np.zeros_like(X_WS)\n",
    "        V_WS_pred = np.zeros_like(X_WS)\n",
    "        P_WS_pred = np.zeros_like(X_WS)\n",
    "        # Values predicted on validation set\n",
    "        U_val_pred = np.zeros_like(X_val)\n",
    "        V_val_pred = np.zeros_like(X_val)\n",
    "        P_val_pred = np.zeros_like(X_val)\n",
    "\n",
    "        for snap in range(0, dim_T_PINN):\n",
    "            t_out = T_PINN[:, snap : snap + 1]\n",
    "            x_out = X_PINN[:, snap : snap + 1]\n",
    "            y_out = Y_PINN[:, snap : snap + 1]\n",
    "\n",
    "            X_out = tf.concat([t_out, x_out, y_out], 1)\n",
    "\n",
    "            # Prediction\n",
    "            Y_out = model(X_out, training = False)\n",
    "            [u_pred_out, v_pred_out, p_pred_out] = tf.split(Y_out, num_or_size_splits = Y_out.shape[1], axis=1)\n",
    "\n",
    "            U_PINN[:,snap : snap + 1] = u_pred_out\n",
    "            V_PINN[:,snap : snap + 1] = v_pred_out\n",
    "            P_PINN[:,snap : snap + 1] = p_pred_out\n",
    "\n",
    "        for snap in range(0, dim_T_WS):\n",
    "            t_out = T_WS[:, snap : snap + 1]\n",
    "            x_out = X_WS[:, snap : snap + 1]\n",
    "            y_out = Y_WS[:, snap : snap + 1]\n",
    "\n",
    "            X_out = tf.concat([t_out, x_out, y_out], 1)\n",
    "\n",
    "            # Prediction\n",
    "            Y_out = model(X_out, training = False)\n",
    "            [u_pred_out, v_pred_out, p_pred_out] = tf.split(Y_out, num_or_size_splits = Y_out.shape[1], axis=1)\n",
    "\n",
    "            U_WS_pred[:,snap : snap + 1] = u_pred_out\n",
    "            V_WS_pred[:,snap : snap + 1] = v_pred_out\n",
    "            P_WS_pred[:,snap : snap + 1] = p_pred_out\n",
    "\n",
    "        for snap in range(0, T_val.shape[1]):\n",
    "            t_out = T_val[:, snap : snap + 1]\n",
    "            x_out = X_val[:, snap : snap + 1]\n",
    "            y_out = Y_val[:, snap : snap + 1]\n",
    "\n",
    "            X_out = tf.concat([t_out, x_out, y_out], 1)\n",
    "\n",
    "            # Prediction\n",
    "            Y_out = model(X_out, training = False)\n",
    "            [u_pred_out, v_pred_out, p_pred_out] = tf.split(Y_out, num_or_size_splits = Y_out.shape[1], axis=1)\n",
    "\n",
    "            U_val_pred[:,snap : snap + 1] = u_pred_out\n",
    "            V_val_pred[:,snap : snap + 1] = v_pred_out\n",
    "            P_val_pred[:,snap : snap + 1] = p_pred_out\n",
    "\n",
    "        # Save data in .mat file (dimensionless units)\n",
    "        scipy.io.savemat('Brussels_%s_lambda_%s_R_%s_envelope.mat' %(str(epoch + 1), str(lamb), str(R)),\n",
    "                            {'T_PINN': T_PINN, 'X_PINN': X_PINN, 'Y_PINN': Y_PINN, 'U_PINN': U_PINN, 'V_PINN': V_PINN, 'P_PINN': P_PINN,\n",
    "                            'T_WS': T_WS, 'X_WS': X_WS, 'Y_WS': Y_WS, 'U_WS': U_WS, 'V_WS': V_WS, 'P_WS': P_WS,\n",
    "                            'U_WS_pred': U_WS_pred, 'V_WS_pred': V_WS_pred, 'P_WS_pred': P_WS_pred,\n",
    "                            'T_val': T_val, 'X_val': X_val, 'Y_val': Y_val, 'U_val': U_val, 'V_val': V_val, 'P_val': P_val,\n",
    "                            'U_val_pred': U_val_pred, 'V_val_pred': V_val_pred, 'P_valt_pred': P_val_pred,\n",
    "                            'Train_loss' : train_loss_results, 'NS_loss' : NS_loss_results,\n",
    "                            'P_loss' : P_loss_results, 'U_loss' : U_loss_results, 'V_loss' : V_loss_results}) # Change ending of the .mat name according to validation case selected: close, far or envelope\n",
    "\n",
    "print('Process completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1747613768647,
     "user": {
      "displayName": "sebastian alejandro gomez ardila",
      "userId": "12249047818400838393"
     },
     "user_tz": 300
    },
    "id": "VtWQ8dSzqFZ1",
    "outputId": "70f3435a-55f8-4658-d98b-0dd08e1bf557"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOsh5vMQ5qRzzxN8h/UOXxM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
