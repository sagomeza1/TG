{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l42CJl98dX_l","outputId":"d4c76592-1272-42fb-d4c7-172d90d5f568","executionInfo":{"status":"ok","timestamp":1747607400792,"user_tz":300,"elapsed":4323,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras==2.12 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n","Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.71.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.13.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.30)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.13.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n","Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n"]}],"source":["!pip install keras==2.12\n","!pip install tensorflow==2.12"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"w3ShbBKDg59R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607402911,"user_tz":300,"elapsed":2113,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"4017c619-6f5d-421c-8fb2-3da4a245ae04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.11/dist-packages (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (24.2)\n","Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (2.13.3)\n"]}],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ga-oXOSubaKr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607408337,"user_tz":300,"elapsed":5427,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"4b125bea-23ff-43aa-94c9-c313695ec570"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using TensorFlow version:  2.12.0 , GPU: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n"]}],"source":["# Import libraries\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]='2'\n","import tensorflow as tf\n","physical_devices = tf.config.list_physical_devices('GPU')\n","availale_GPUs = len(physical_devices)\n","print('Using TensorFlow version: ', tf.__version__, ', GPU:', availale_GPUs)\n","if physical_devices:\n","    try:\n","        for gpu in physical_devices:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)\n","import numpy as np\n","import scipy.io\n","from tensorflow.keras.layers import Input, Activation, Dense\n","from tensorflow.keras.models import Model\n","import pandas as pd\n","import datetime as dt\n","import tensorflow_addons as tfa"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rg7qN1qDdHr4","executionInfo":{"status":"ok","timestamp":1747607408608,"user_tz":300,"elapsed":267,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Extract data from dataset\n","\n","\n","WS_data = scipy.io.loadmat('Weather_data.mat')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FPSxJ3EHe4yS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607429463,"user_tz":300,"elapsed":20847,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"a3676097-33ab-4315-912e-537b8508ed4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Double-check for NaN in time sequence 0\n"]}],"source":["# Convert date to continuous time: from date and time format to seconds\n","date_0 = WS_data['Date'][0]\n","date = []\n","for i in range(0, len(date_0)):\n","    date = np.append(date, str(date_0[i])[2 : -2])\n","time_init = dt.datetime(int(date[0][0 : 4]), int(date[0][5 : 7]), int(date[0][8 : 10]), int(date[0][11 : 13]), int(date[0][14 : 16]))\n","T_nan_index = np.argwhere(pd.isna(date))\n","date = np.delete(date, T_nan_index[:, 0],  0)\n","print('Double-check for NaN in time sequence', np.sum(pd.isna(date)))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"A71pYdO6iHKl","executionInfo":{"status":"ok","timestamp":1747607429580,"user_tz":300,"elapsed":115,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["Seconds = np.zeros((date.shape[0], 1))\n","for index in range(date.shape[0]):\n","    Seconds[index, 0] = ((dt.datetime(int(date[index][0 : 4]), int(date[index][5 : 7]), int(date[index][8 : 10]), int(date[index][11 : 13]), int(date[index][14 : 16])) - time_init).total_seconds())\n","T_WS = Seconds"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-t7tEYSehVUX","executionInfo":{"status":"ok","timestamp":1747607429593,"user_tz":300,"elapsed":11,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Convert to Cartesian coordinates\n","X_WS = np.array(6378000 * np.sin(np.radians(WS_data['Lon'])))[0]  # Longitude to meters\n","Y_WS = np.array(6378000 * np.sin(np.radians(WS_data['Lat'])))[0]  # Latitude to meters\n","Z_WS = np.array(WS_data['Alt'])[0]\n","Temp_WS = np.array(WS_data['Temperature'])[0]"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KqbAX-DiipIP","executionInfo":{"status":"ok","timestamp":1747607429605,"user_tz":300,"elapsed":9,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Project wind speed and direction into Cartesian coordinates\n","U_WS = (WS_data['WindSpeed'] * WS_data['WindDirectionX'])[0]\n","V_WS = (WS_data['WindSpeed'] * WS_data['WindDirectionY'])[0]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dCmCdCOFk345","executionInfo":{"status":"ok","timestamp":1747607429608,"user_tz":300,"elapsed":1,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Pressure from mbar to Pa\n","P_WS = WS_data['Pressure'][0] * 100"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"r4JYL4Mak5db","executionInfo":{"status":"ok","timestamp":1747607429621,"user_tz":300,"elapsed":12,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Remove NaN values from time field\n","X_WS = np.delete(X_WS, T_nan_index[:, 0],  0)\n","Y_WS = np.delete(Y_WS, T_nan_index[:, 0],  0)\n","Z_WS = np.delete(Z_WS, T_nan_index[:, 0],  0)\n","U_WS = np.delete(U_WS, T_nan_index[:, 0],  0)\n","V_WS = np.delete(V_WS, T_nan_index[:, 0],  0)\n","P_WS = np.delete(P_WS, T_nan_index[:, 0],  0)\n","Temp_WS = np.delete(Temp_WS, T_nan_index[:, 0],  0)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"arpzY1xVk7Kg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607429685,"user_tz":300,"elapsed":62,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"db9ab190-ddc6-4c47-a080-04ecd413feab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of weather stations: 21\n"]}],"source":["# Structure data into matrix: 21 available stations (rows) x measurement every 10 min (column)\n","T_WS = np.reshape(T_WS, (int(T_WS.shape[0] / 21), 21)).T # There are 21 WS stations in this case\n","X_WS = np.reshape(X_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","Y_WS = np.reshape(Y_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","Z_WS = np.reshape(Z_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","U_WS = np.reshape(U_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","V_WS = np.reshape(V_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","P_WS = np.reshape(P_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","Temp_WS = np.reshape(Temp_WS, (T_WS.shape[1], T_WS.shape[0])).T\n","print('Number of weather stations:', T_WS.shape[0])"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DS0-9D-Uk9Ck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607429686,"user_tz":300,"elapsed":21,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"c7dde395-1567-47f4-907f-8834808e63b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Double-check for NaN in location field 0\n"]}],"source":["# Remove NaN from location data\n","X_nan_index = np.argwhere(np.isnan(X_WS))\n","T_WS = np.delete(T_WS, X_nan_index[:, 0],  0)\n","P_WS = np.delete(P_WS, X_nan_index[:, 0],  0)\n","U_WS = np.delete(U_WS, X_nan_index[:, 0],  0)\n","V_WS = np.delete(V_WS, X_nan_index[:, 0],  0)\n","X_WS = np.delete(X_WS, X_nan_index[:, 0],  0)\n","Y_WS = np.delete(Y_WS, X_nan_index[:, 0],  0)\n","Z_WS = np.delete(Z_WS, X_nan_index[:, 0],  0)\n","Temp_WS = np.delete(Temp_WS, X_nan_index[:, 0],  0)\n","print('Double-check for NaN in location field', np.sum(np.isnan(X_WS)))"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"JusIIBFRlBJc","executionInfo":{"status":"ok","timestamp":1747607429686,"user_tz":300,"elapsed":16,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Days selected for reconstruction\n","n_days = 14 # Change up to a maximum of 14 availsble days\n","samples =  int(144 * n_days) # Convert selected days to snapshots\n","T_WS = T_WS[:, : samples]\n","X_WS = X_WS[:, : samples]\n","Y_WS = Y_WS[:, : samples]\n","Z_WS = Z_WS[:, : samples]\n","U_WS = U_WS[:, : samples]\n","V_WS = V_WS[:, : samples]\n","P_WS = P_WS[:, : samples]\n","Temp_WS = Temp_WS[:, : samples]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Z9hA23PwlFZ_","executionInfo":{"status":"ok","timestamp":1747607429694,"user_tz":300,"elapsed":14,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Sort values in matrix into increasing values of X coordinate\n","for snap in range(0, T_WS.shape[1]):\n","    index_sort = np.argsort(X_WS[:, snap])\n","    T_WS[:, snap] = T_WS[index_sort, snap]\n","    X_WS[:, snap] = X_WS[index_sort, snap]\n","    Y_WS[:, snap] = Y_WS[index_sort, snap]\n","    Z_WS[:, snap] = Z_WS[index_sort, snap]\n","    U_WS[:, snap] = U_WS[index_sort, snap]\n","    V_WS[:, snap] = V_WS[index_sort, snap]\n","    P_WS[:, snap] = P_WS[index_sort, snap]\n","    Temp_WS[:, snap] = Temp_WS[index_sort, snap]"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"3R7HdeBelHj7","executionInfo":{"status":"ok","timestamp":1747607429711,"user_tz":300,"elapsed":17,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Delete NaN from U, V and P if constantly occuring for each weather station\n","uvp_mean = np.nanmean(np.concatenate([U_WS, V_WS, P_WS], axis = 1), axis = 1)[:, None]\n","vel_nan_index = np.argwhere(np.isnan(uvp_mean))\n","T_WS = np.delete(T_WS, vel_nan_index[:, 0],  0)\n","P_WS = np.delete(P_WS, vel_nan_index[:, 0],  0)\n","U_WS = np.delete(U_WS, vel_nan_index[:, 0],  0)\n","V_WS = np.delete(V_WS, vel_nan_index[:, 0],  0)\n","X_WS = np.delete(X_WS, vel_nan_index[:, 0],  0)\n","Y_WS = np.delete(Y_WS, vel_nan_index[:, 0],  0)\n","Z_WS = np.delete(Z_WS, vel_nan_index[:, 0],  0)\n","Temp_WS = np.delete(Temp_WS, vel_nan_index[:, 0],  0)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"uzijh35TlJOf","executionInfo":{"status":"ok","timestamp":1747607429712,"user_tz":300,"elapsed":12,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Correct pressure to sea level (ISA)\n","P_WS = P_WS * (1 - 0.0065 * Z_WS / (Temp_WS + 273.15 + 0.0065 * Z_WS))**(-5.257)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pFrtOAEMlLCT","executionInfo":{"status":"ok","timestamp":1747607429713,"user_tz":300,"elapsed":4,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Certering of location and time fields\n","x_min = np.min(X_WS)\n","x_max = np.max(X_WS)\n","X_WS = X_WS - (x_min + x_max) / 2\n","y_min = np.min(Y_WS)\n","y_max = np.max(Y_WS)\n","Y_WS = Y_WS - (y_min + y_max) / 2\n","t_min = np.min(T_WS)\n","t_max = np.max(T_WS)\n","T_WS = T_WS - t_min # Refer to t = 0"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"VtS2XKrqlMwj","executionInfo":{"status":"ok","timestamp":1747607429725,"user_tz":300,"elapsed":5,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# PINN output grid\n","T_PINN = T_WS[0 : 1, :] # Same times for reconstruction"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ijwNJ1smlPAI","executionInfo":{"status":"ok","timestamp":1747607429725,"user_tz":300,"elapsed":3,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Resolution in degrees\n","R = 0.2\n","R_PINN = 6378000 * np.sin(np.radians(R)) # Grid resolution\n","x_PINN = np.arange(x_min - R_PINN, x_max + R_PINN, R_PINN) # X values in output resolution\n","y_PINN = np.arange(y_min - R_PINN, y_max + R_PINN, R_PINN) # Y values in output resolution"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"6JAa9AZ9lRAw","executionInfo":{"status":"ok","timestamp":1747607429726,"user_tz":300,"elapsed":3,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Centering of location data\n","x_PINN = x_PINN - (x_min + x_max) / 2\n","y_PINN = y_PINN - (y_min + y_max) / 2"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"xSlCDKxplTMR","executionInfo":{"status":"ok","timestamp":1747607429780,"user_tz":300,"elapsed":6,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Final output grid\n","X_PINN, Y_PINN = np.meshgrid(x_PINN, y_PINN)\n","X_PINN = X_PINN.flatten('F')[:, None]\n","Y_PINN = Y_PINN.flatten('F')[:, None]"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"pV6Miw43lU85","executionInfo":{"status":"ok","timestamp":1747607429791,"user_tz":300,"elapsed":12,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Dimensions\n","dim_T_PINN = T_PINN.shape[1]\n","dim_N_PINN = X_PINN.shape[0]\n","\n","T_PINN = np.tile(T_PINN, (dim_N_PINN, 1))\n","X_PINN = np.tile(X_PINN, dim_T_PINN)\n","Y_PINN = np.tile(Y_PINN, dim_T_PINN)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"bf-ue-Dwla_V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607429792,"user_tz":300,"elapsed":11,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"84822777-a17e-4d53-9f69-20c8ba7116cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["L: 409791.6159110926 W 16.718180196652117 P0 100359.48707895375 Re 495728659759\n"]}],"source":["# Reference values for non-dimensionalization\n","L = np.sqrt((x_max - x_min) ** 2 + (y_max - y_min) ** 2) # Reference distance\n","W = np.sqrt(np.nanmax(abs(U_WS)) ** 2 + np.nanmax(abs(V_WS)) ** 2) # Reference velocity\n","rho = 1.269 # Air density at 15 degrees\n","nu = 1.382e-5 # Kinematic viscosity at 15 degrees\n","Re = int(W * L / nu) # Reynolds number\n","P0 = np.nanmean(P_WS) # Reference pressure level\n","print('L:', L, 'W', W, 'P0', P0, 'Re', Re)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"IL_1xz60lfBO","executionInfo":{"status":"ok","timestamp":1747607429793,"user_tz":300,"elapsed":7,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Non-dimensionalization\n","X_WS = X_WS / L\n","Y_WS = Y_WS / L\n","T_WS = T_WS * W / L\n","P_WS = (P_WS - P0) / rho / (W ** 2)\n","U_WS = U_WS / W\n","V_WS = V_WS / W\n","\n","X_PINN = X_PINN / L\n","Y_PINN = Y_PINN / L\n","T_PINN = T_PINN * W / L"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"_Xb23ymRlihf","executionInfo":{"status":"ok","timestamp":1747607429793,"user_tz":300,"elapsed":5,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Validation cases (remove stations)\n","# # N_test = 0 # Number of stations to remove\n","WS_val = np.array([1, 2, 3, 5, 7, 9, 10, 11, 13, 14, 15, 16, 19])\n","# Choose between different arrays for desired validation case:\n","# Close: np.array([2, 8, 10, 14, 16, 19])\n","# Far: np.array([0, 1, 4, 6, 8, 12, 17, 18, 19, 20])\n","# Envelope: np.array([1, 2, 3, 5, 7, 9, 10, 11, 13, 14, 15, 16, 19])"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"geJyAoGhlgsi","executionInfo":{"status":"ok","timestamp":1747607429794,"user_tz":300,"elapsed":5,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Remove WS for validation\n","T_val = T_WS[WS_val, :]\n","P_val = P_WS[WS_val, :]\n","U_val = U_WS[WS_val, :]\n","V_val = V_WS[WS_val, :]\n","X_val = X_WS[WS_val, :]\n","Y_val = Y_WS[WS_val, :]\n","Z_val = Z_WS[WS_val, :]"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"vWeZ9iUUlkcW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607429800,"user_tz":300,"elapsed":10,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"431151a3-90cb-4549-a92d-bace232a4dac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of final weather stations available for training: 8\n"]}],"source":["# Remaining Ws for training\n","T_WS = np.delete(T_WS, WS_val, 0)\n","P_WS = np.delete(P_WS, WS_val, 0)\n","U_WS = np.delete(U_WS, WS_val, 0)\n","V_WS = np.delete(V_WS, WS_val, 0)\n","X_WS = np.delete(X_WS, WS_val, 0)\n","Y_WS = np.delete(Y_WS, WS_val, 0)\n","print('Number of final weather stations available for training:', T_WS.shape[0])"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"wJQyrCXelmKK","executionInfo":{"status":"ok","timestamp":1747607429802,"user_tz":300,"elapsed":1,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Dimensions\n","dim_N_WS = X_WS.shape[0]\n","dim_T_WS = X_WS.shape[1]\n","\n","del WS_data"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"U1363lN5loOT","executionInfo":{"status":"ok","timestamp":1747607429815,"user_tz":300,"elapsed":11,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Customized dense layer\n","class GammaBiasLayer(tf.keras.layers.Layer):\n","    def __init__(self, units, *args, **kwargs):\n","        super(GammaBiasLayer, self).__init__(*args, **kwargs)\n","        self.units = units\n","\n","    def build(self, input_shape):\n","        self.bias = self.add_weight('bias',\n","                                    shape=(self.units,),\n","                                    initializer='zeros',\n","                                    trainable=True)\n","\n","        self.gamma = self.add_weight('gamma',\n","                                     shape = (self.units,),\n","                                     initializer = 'ones',\n","                                     trainable = True)\n","\n","        self.w = tfa.layers.WeightNormalization(Dense(self.units, use_bias = False,\n","                                    kernel_initializer = tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None),\n","                                    trainable = True, activation = None))\n","\n","\n","    def call(self, input_tensor):\n","        return self.gamma * self.w(input_tensor) + self.bias"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"dZpl26l3lqJU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747607431368,"user_tz":300,"elapsed":1551,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"13a6f860-f1d1-4f33-c449-1a8836b1c14e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 3)]               0         \n","                                                                 \n"," gamma_bias_layer (GammaBias  (None, 600)              5401      \n"," Layer)                                                          \n","                                                                 \n"," activation (Activation)     (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_1 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," activation_1 (Activation)   (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_2 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," activation_2 (Activation)   (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_3 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," activation_3 (Activation)   (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_4 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," activation_4 (Activation)   (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_5 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," activation_5 (Activation)   (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_6 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," activation_6 (Activation)   (None, 600)               0         \n","                                                                 \n"," gamma_bias_layer_7 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," gamma_bias_layer_8 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," gamma_bias_layer_9 (GammaBi  (None, 600)              721801    \n"," asLayer)                                                        \n","                                                                 \n"," gamma_bias_layer_10 (GammaB  (None, 600)              721801    \n"," iasLayer)                                                       \n","                                                                 \n"," gamma_bias_layer_11 (GammaB  (None, 600)              721801    \n"," iasLayer)                                                       \n","                                                                 \n"," gamma_bias_layer_12 (GammaB  (None, 3)                3610      \n"," iasLayer)                                                       \n","                                                                 \n","=================================================================\n","Total params: 7,948,822\n","Trainable params: 3,985,209\n","Non-trainable params: 3,963,613\n","_________________________________________________________________\n"]}],"source":["# Model\n","num_input_variables = 3 # t, x, y\n","num_output_variables = 3 # u, v, p\n","\n","neurons = 200 * num_output_variables\n","layers = [num_input_variables] + (2 * (num_input_variables + num_output_variables))*[neurons] + [num_output_variables]\n","\n","inputs = Input(shape = (num_input_variables, ))\n","h = GammaBiasLayer(layers[1])(inputs)\n","h = Activation('tanh')(h)\n","for l in layers[2 : 2 * int((len(layers) - 2) / 3)]:\n","    h = GammaBiasLayer(l)(h)\n","    h = Activation('tanh')(h)\n","for l in layers[2 * int((len(layers) - 2) / 3) : -1]:\n","    h = GammaBiasLayer(layers[-2])(h)\n","outputs = GammaBiasLayer(layers[-1])(h)\n","\n","model = Model(inputs = inputs, outputs = outputs)\n","\n","model.summary()"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"tc4N93WWpvf1","executionInfo":{"status":"ok","timestamp":1747607431372,"user_tz":300,"elapsed":2,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Error functions and loss function\n","mse = tf.keras.losses.MeanSquaredError()\n","rmse = tf.keras.metrics.RootMeanSquaredError()\n","\n","@tf.function(reduce_retracing = True)\n","def loss_NS_2D(model, t_eqns_batch, x_eqns_batch, y_eqns_batch, training):\n","    mse = tf.keras.losses.MeanSquaredError()\n","    with tf.GradientTape(persistent = True) as tape1:\n","        tape1.watch((t_eqns_batch, x_eqns_batch, y_eqns_batch))\n","        X_eqns_batch = tf.concat([t_eqns_batch, x_eqns_batch, y_eqns_batch], axis = 1)\n","        Y_eqns_batch = model(X_eqns_batch, training = training)\n","        [u_eqns_pred, v_eqns_pred, p_eqns_pred] = tf.split(Y_eqns_batch, num_or_size_splits=Y_eqns_batch.shape[1], axis=1)\n","\n","    # Derivatives\n","    u_t_eqns_pred = tape1.gradient(u_eqns_pred, t_eqns_batch)\n","    v_t_eqns_pred = tape1.gradient(v_eqns_pred, t_eqns_batch)\n","\n","    u_x_eqns_pred = tape1.gradient(u_eqns_pred, x_eqns_batch)\n","    v_x_eqns_pred = tape1.gradient(v_eqns_pred, x_eqns_batch)\n","    p_x_eqns_pred = tape1.gradient(p_eqns_pred, x_eqns_batch)\n","\n","    u_y_eqns_pred = tape1.gradient(u_eqns_pred, y_eqns_batch)\n","    v_y_eqns_pred = tape1.gradient(v_eqns_pred, y_eqns_batch)\n","    p_y_eqns_pred = tape1.gradient(p_eqns_pred, y_eqns_batch)\n","\n","    # Navier-Stokes residuals\n","    e1 = (u_x_eqns_pred + v_y_eqns_pred)\n","    e2 = (u_t_eqns_pred + (u_eqns_pred * u_x_eqns_pred + v_eqns_pred * u_y_eqns_pred) + p_x_eqns_pred)\n","    e3 = (v_t_eqns_pred + (u_eqns_pred * v_x_eqns_pred + v_eqns_pred * v_y_eqns_pred) + p_y_eqns_pred)\n","\n","    return mse(0, e1) + mse(0, e2) + mse(0, e3)\n","\n","def loss_u(model, t_data_batch, x_data_batch, y_data_batch, u_data_batch, training):\n","    mse = tf.keras.losses.MeanSquaredError()\n","    X_data_batch = tf.concat([t_data_batch, x_data_batch, y_data_batch], axis = 1)\n","    Y_data_batch = model(X_data_batch, training = training)\n","    [u_data_pred, _, _] = tf.split(Y_data_batch, num_or_size_splits=Y_data_batch.shape[1], axis=1)\n","\n","    return mse(u_data_batch, u_data_pred) / tf.math.reduce_std(u_data_batch)**2\n","\n","def loss_v(model, t_data_batch, x_data_batch, y_data_batch, v_data_batch, training):\n","    mse = tf.keras.losses.MeanSquaredError()\n","    X_data_batch = tf.concat([t_data_batch, x_data_batch, y_data_batch], axis = 1)\n","    Y_data_batch = model(X_data_batch, training = training)\n","    [_, v_data_pred, _] = tf.split(Y_data_batch, num_or_size_splits=Y_data_batch.shape[1], axis=1)\n","\n","    return mse(v_data_batch, v_data_pred) / tf.math.reduce_std(v_data_batch)**2\n","\n","def loss_p(model, t_data_batch, x_data_batch, y_data_batch, p_data_batch, training):\n","    mse = tf.keras.losses.MeanSquaredError()\n","    X_data_batch = tf.concat([t_data_batch, x_data_batch, y_data_batch], axis = 1)\n","    Y_data_batch = model(X_data_batch, training = training)\n","    [_, _, p_data_pred] = tf.split(Y_data_batch, num_or_size_splits=Y_data_batch.shape[1], axis=1)\n","\n","    return mse(p_data_batch, p_data_pred) / tf.math.reduce_std(p_data_batch)**2\n","\n","def loss_total(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch, t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb, training):\n","    NS_eqns = lamb * loss_NS_2D(model, t_eqns_batch, x_eqns_batch, y_eqns_batch, training)\n","    NS_data = lamb * loss_NS_2D(model, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, training)\n","    P_e = loss_p(model, t_p_batch, x_p_batch, y_p_batch, p_p_batch, training)\n","    U_e = loss_u(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, training)\n","    V_e = loss_v(model, t_v_batch, x_v_batch, y_v_batch, v_v_batch, training)\n","\n","    total_e = NS_eqns + NS_data + U_e + V_e + P_e\n","\n","    return  (NS_eqns ** 2 + NS_data**2 + U_e ** 2 + V_e ** 2 + P_e ** 2) / total_e"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"6jY1RckVp2fV","executionInfo":{"status":"ok","timestamp":1747607431376,"user_tz":300,"elapsed":2,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Optimize model - gradients:\n","def grad(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch,  t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb):\n","    with tf.GradientTape() as tape:\n","        loss_value = loss_total(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch,  t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb, training = True)\n","    gradient_model = tape.gradient(loss_value, model.trainable_variables)\n","    return loss_value, gradient_model"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Bd-SXtTtp5eI","executionInfo":{"status":"ok","timestamp":1747607431405,"user_tz":300,"elapsed":19,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Create an optimizer\n","model_optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"pGUwmAuQp7F-","executionInfo":{"status":"ok","timestamp":1747607431408,"user_tz":300,"elapsed":20,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Keep results for plotting\n","train_loss_results = []\n","NS_loss_results = []\n","P_loss_results = []\n","U_loss_results = []\n","V_loss_results = []"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"xetSaHmFp9hj","executionInfo":{"status":"ok","timestamp":1747607431409,"user_tz":300,"elapsed":19,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Training\n","num_epochs = 5 # number of epochs\n","# num_epochs = 1000 # number of epochs\n","lamb = 2 # Tuning of physics constraints\n","batch_PINN = int(np.ceil((dim_N_PINN * dim_T_PINN / n_days * R)))\n","batch_WS = int(np.ceil(dim_N_WS * dim_T_WS / n_days * R))"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"_M_V92Kkp_Jk","executionInfo":{"status":"ok","timestamp":1747607431411,"user_tz":300,"elapsed":19,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[],"source":["# Data dimensions\n","dim_N_data = dim_N_WS\n","dim_T_data = dim_T_WS\n","dim_T_eqns = dim_T_PINN\n","dim_N_eqns = dim_N_PINN"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RO7BUA4rqAlH","outputId":"d0cce82a-8df1-4ade-d0cf-13cb83d8eff4","executionInfo":{"status":"ok","timestamp":1747613768645,"user_tz":300,"elapsed":6337251,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0 Loss_training: 2.657e+01 NS_loss: 7.183e-01 P_loss: 1.014e+00 U_loss: 2.712e+00 V_loss: 2.803e+01\n","Epoch: 1 Loss_training: 6.855e+00 NS_loss: 5.125e-01 P_loss: 9.401e-01 U_loss: 3.993e+00 V_loss: 6.395e+00\n","Epoch: 2 Loss_training: 1.832e+00 NS_loss: 2.666e-01 P_loss: 8.440e-01 U_loss: 1.858e+00 V_loss: 1.477e+00\n","Epoch: 3 Loss_training: 2.547e+00 NS_loss: 3.632e-01 P_loss: 6.078e-01 U_loss: 2.042e+00 V_loss: 2.761e+00\n","Epoch: 4 Loss_training: 1.268e+00 NS_loss: 2.080e-01 P_loss: 5.904e-01 U_loss: 1.275e+00 V_loss: 1.259e+00\n","Process completed\n"]}],"source":["for epoch in range(num_epochs):\n","    epoch_loss_avg = tf.keras.metrics.Mean()\n","    epoch_NS_loss_avg = tf.keras.metrics.Mean()\n","    epoch_P_loss_avg = tf.keras.metrics.Mean()\n","    epoch_U_loss_avg = tf.keras.metrics.Mean()\n","    epoch_V_loss_avg = tf.keras.metrics.Mean()\n","\n","    # Data mixing and shuffling\n","    idx_t = np.random.choice(dim_T_WS, dim_T_data, replace = False)\n","    idx_x = np.random.choice(dim_N_WS, dim_N_data, replace = False)\n","    t_u = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    x_u = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    y_u = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    z_u = Z_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    u_u = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    v_u = V_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    p_u = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","\n","    idx_t = np.random.choice(dim_T_WS, dim_T_data, replace = False)\n","    idx_x = np.random.choice(dim_N_WS, dim_N_data, replace = False)\n","    t_v = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    x_v = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    y_v = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    z_v = Z_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    u_v = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    v_v = V_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    p_v = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","\n","    idx_t = np.random.choice(P_WS.shape[1], P_WS.shape[1], replace = False)\n","    idx_x = np.random.choice(P_WS.shape[0], P_WS.shape[0], replace = False)\n","    t_p = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    x_p = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    y_p = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    z_p = Z_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    u_p = U_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    v_p = V_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    p_p = P_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","\n","    idx_t = np.random.choice(dim_T_PINN, dim_T_eqns, replace = False)\n","    idx_x = np.random.choice(dim_N_PINN, dim_N_eqns, replace = False)\n","    t_eqns = T_PINN[:, idx_t][idx_x,:].flatten()[:,None]\n","    x_eqns = X_PINN[:, idx_t][idx_x,:].flatten()[:,None]\n","    y_eqns = Y_PINN[:, idx_t][idx_x,:].flatten()[:,None]\n","\n","    idx_t = np.random.choice(dim_T_WS, dim_T_data, replace = False)\n","    idx_x = np.random.choice(dim_N_WS, dim_N_data, replace = False)\n","    t_eqns_ref = T_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    x_eqns_ref = X_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","    y_eqns_ref = Y_WS[:, idx_t][idx_x,:].flatten()[:,None]\n","\n","    idx_batch = np.random.choice(t_u.shape[0], t_u.shape[0], replace = False)\n","    t_u = t_u[idx_batch, :]\n","    x_u = x_u[idx_batch, :]\n","    y_u = y_u[idx_batch, :]\n","    u_u = u_u[idx_batch, :]\n","    idx_batch = np.random.choice(t_v.shape[0], t_v.shape[0], replace = False)\n","    t_v = t_v[idx_batch, :]\n","    x_v = x_v[idx_batch, :]\n","    y_v = y_v[idx_batch, :]\n","    v_v = v_v[idx_batch, :]\n","    idx_batch = np.random.choice(t_p.shape[0], t_p.shape[0], replace = False)\n","    t_p = t_p[idx_batch, :]\n","    x_p = x_p[idx_batch, :]\n","    y_p = y_p[idx_batch, :]\n","    p_p = p_p[idx_batch, :]\n","    idx_batch = np.random.choice(t_eqns.shape[0], t_eqns.shape[0], replace = False)\n","    t_eqns = t_eqns[idx_batch, :]\n","    x_eqns = x_eqns[idx_batch, :]\n","    y_eqns = y_eqns[idx_batch, :]\n","    idx_batch = np.random.choice(t_eqns_ref.shape[0], t_eqns_ref.shape[0], replace = False)\n","    t_eqns_ref = t_eqns_ref[idx_batch, :]\n","    x_eqns_ref = x_eqns_ref[idx_batch, :]\n","    y_eqns_ref = y_eqns_ref[idx_batch, :]\n","\n","    # Remove remaining NaN\n","    nan_index = np.argwhere(np.isnan(u_u))\n","    t_u = np.delete(t_u, nan_index[:, 0],  0)\n","    u_u = np.delete(u_u, nan_index[:, 0],  0)\n","    x_u = np.delete(x_u, nan_index[:, 0],  0)\n","    y_u = np.delete(y_u, nan_index[:, 0],  0)\n","    nan_index = np.argwhere(np.isnan(v_v))\n","    t_v = np.delete(t_v, nan_index[:, 0],  0)\n","    v_v = np.delete(v_v, nan_index[:, 0],  0)\n","    x_v = np.delete(x_v, nan_index[:, 0],  0)\n","    y_v = np.delete(y_v, nan_index[:, 0],  0)\n","    nan_index = np.argwhere(np.isnan(p_p))\n","    t_p = np.delete(t_p, nan_index[:, 0],  0)\n","    p_p = np.delete(p_p, nan_index[:, 0],  0)\n","    x_p = np.delete(x_p, nan_index[:, 0],  0)\n","    y_p = np.delete(y_p, nan_index[:, 0],  0)\n","\n","    # Batch size distribution\n","    div_u = range(0, len(x_u), batch_WS)\n","    div_v = range(0, len(x_v), batch_WS)\n","    div_p = range(0, len(x_p), batch_WS)\n","    div_eqns = range(0, len(x_eqns_ref), batch_WS)\n","    div_PINN = range(0, len(x_eqns), batch_PINN)\n","\n","    min_div = min([len(div_u), len(div_v), len(div_p), len(div_eqns), len(div_PINN)])\n","\n","    # Batch step\n","    for index in range(0, min_div):\n","        index_u = div_u[index]\n","        index_v = div_v[index]\n","        index_p = div_p[index]\n","        index_eqns = div_eqns[index]\n","        index_PINN = div_PINN[index]\n","        t_u_batch = tf.convert_to_tensor(t_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n","        x_u_batch = tf.convert_to_tensor(x_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n","        y_u_batch = tf.convert_to_tensor(y_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n","        u_u_batch = tf.convert_to_tensor(u_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n","        v_u_batch = tf.convert_to_tensor(v_u[index_u : index_u + batch_WS, :], dtype = 'float32')\n","        t_v_batch = tf.convert_to_tensor(t_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n","        x_v_batch = tf.convert_to_tensor(x_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n","        y_v_batch = tf.convert_to_tensor(y_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n","        u_v_batch = tf.convert_to_tensor(u_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n","        v_v_batch = tf.convert_to_tensor(v_v[index_v : index_v + batch_WS, :], dtype = 'float32')\n","        t_p_batch = tf.convert_to_tensor(t_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n","        x_p_batch = tf.convert_to_tensor(x_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n","        y_p_batch = tf.convert_to_tensor(y_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n","        u_p_batch = tf.convert_to_tensor(u_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n","        v_p_batch = tf.convert_to_tensor(v_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n","        p_p_batch = tf.convert_to_tensor(p_p[index_p : index_p + batch_WS, :], dtype = 'float32')\n","        t_eqns_ref_batch = tf.convert_to_tensor(t_eqns_ref[index_eqns : index_eqns + batch_WS, :], dtype = 'float32')\n","        x_eqns_ref_batch = tf.convert_to_tensor(x_eqns_ref[index_eqns : index_eqns + batch_WS, :], dtype = 'float32')\n","        y_eqns_ref_batch = tf.convert_to_tensor(y_eqns_ref[index_eqns : index_eqns + batch_WS, :], dtype = 'float32')\n","        t_eqns_batch = tf.convert_to_tensor(t_eqns[index_PINN : index_PINN + batch_PINN, :], dtype = 'float32')\n","        x_eqns_batch = tf.convert_to_tensor(x_eqns[index_PINN : index_PINN + batch_PINN, :], dtype = 'float32')\n","        y_eqns_batch = tf.convert_to_tensor(y_eqns[index_PINN : index_PINN + batch_PINN, :], dtype = 'float32')\n","\n","        loss_train, grads = grad(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, t_v_batch, x_v_batch, y_v_batch, v_v_batch,  t_p_batch, x_p_batch, y_p_batch, p_p_batch, t_eqns_ref_batch, x_eqns_ref_batch, y_eqns_ref_batch, t_eqns_batch, x_eqns_batch, y_eqns_batch, lamb)\n","\n","        NS_loss = loss_NS_2D(model, t_eqns_batch, x_eqns_batch, y_eqns_batch, training = False)\n","        P_loss = loss_p(model, t_p_batch, x_p_batch, y_p_batch, p_p_batch, training = False)\n","        U_loss = loss_u(model, t_u_batch, x_u_batch, y_u_batch, u_u_batch, training = False)\n","        V_loss = loss_v(model, t_v_batch, x_v_batch, y_v_batch, v_v_batch, training = False)\n","\n","        model_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        epoch_loss_avg.update_state(loss_train)\n","        epoch_NS_loss_avg.update_state(NS_loss)\n","        epoch_P_loss_avg.update_state(P_loss)\n","        epoch_U_loss_avg.update_state(U_loss)\n","        epoch_V_loss_avg.update_state(V_loss)\n","\n","    # End epoch\n","    train_loss_results.append(epoch_loss_avg.result())\n","    NS_loss_results.append(epoch_NS_loss_avg.result())\n","    P_loss_results.append(epoch_P_loss_avg.result())\n","    U_loss_results.append(epoch_U_loss_avg.result())\n","    V_loss_results.append(epoch_V_loss_avg.result())\n","\n","    # Update learning rate (adaptive)\n","    if epoch_loss_avg.result() > 1e-1:\n","        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","    elif epoch_loss_avg.result() > 3e-2:\n","        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n","    elif epoch_loss_avg.result() > 3e-3:\n","        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","    else:\n","        model_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n","\n","    print(\"Epoch: {:d} Loss_training: {:.3e} NS_loss: {:.3e} P_loss: {:.3e} U_loss: {:.3e} V_loss: {:.3e}\".format(epoch, epoch_loss_avg.result(),\n","    epoch_NS_loss_avg.result(), epoch_P_loss_avg.result(), epoch_U_loss_avg.result(), epoch_V_loss_avg.result()))\n","\n","    ################# Save Data ###########################\n","    if (epoch + 1) % num_epochs == 0:\n","        # Output in higher resolution\n","        U_PINN = np.zeros_like(X_PINN)\n","        V_PINN = np.zeros_like(X_PINN)\n","        P_PINN = np.zeros_like(X_PINN)\n","        # Values predicted on WS locations\n","        U_WS_pred = np.zeros_like(X_WS)\n","        V_WS_pred = np.zeros_like(X_WS)\n","        P_WS_pred = np.zeros_like(X_WS)\n","        # Values predicted on validation set\n","        U_val_pred = np.zeros_like(X_val)\n","        V_val_pred = np.zeros_like(X_val)\n","        P_val_pred = np.zeros_like(X_val)\n","\n","        for snap in range(0, dim_T_PINN):\n","            t_out = T_PINN[:, snap : snap + 1]\n","            x_out = X_PINN[:, snap : snap + 1]\n","            y_out = Y_PINN[:, snap : snap + 1]\n","\n","            X_out = tf.concat([t_out, x_out, y_out], 1)\n","\n","            # Prediction\n","            Y_out = model(X_out, training = False)\n","            [u_pred_out, v_pred_out, p_pred_out] = tf.split(Y_out, num_or_size_splits = Y_out.shape[1], axis=1)\n","\n","            U_PINN[:,snap : snap + 1] = u_pred_out\n","            V_PINN[:,snap : snap + 1] = v_pred_out\n","            P_PINN[:,snap : snap + 1] = p_pred_out\n","\n","        for snap in range(0, dim_T_WS):\n","            t_out = T_WS[:, snap : snap + 1]\n","            x_out = X_WS[:, snap : snap + 1]\n","            y_out = Y_WS[:, snap : snap + 1]\n","\n","            X_out = tf.concat([t_out, x_out, y_out], 1)\n","\n","            # Prediction\n","            Y_out = model(X_out, training = False)\n","            [u_pred_out, v_pred_out, p_pred_out] = tf.split(Y_out, num_or_size_splits = Y_out.shape[1], axis=1)\n","\n","            U_WS_pred[:,snap : snap + 1] = u_pred_out\n","            V_WS_pred[:,snap : snap + 1] = v_pred_out\n","            P_WS_pred[:,snap : snap + 1] = p_pred_out\n","\n","        for snap in range(0, T_val.shape[1]):\n","            t_out = T_val[:, snap : snap + 1]\n","            x_out = X_val[:, snap : snap + 1]\n","            y_out = Y_val[:, snap : snap + 1]\n","\n","            X_out = tf.concat([t_out, x_out, y_out], 1)\n","\n","            # Prediction\n","            Y_out = model(X_out, training = False)\n","            [u_pred_out, v_pred_out, p_pred_out] = tf.split(Y_out, num_or_size_splits = Y_out.shape[1], axis=1)\n","\n","            U_val_pred[:,snap : snap + 1] = u_pred_out\n","            V_val_pred[:,snap : snap + 1] = v_pred_out\n","            P_val_pred[:,snap : snap + 1] = p_pred_out\n","\n","        # Save data in .mat file (dimensionless units)\n","        scipy.io.savemat('Brussels_%s_lambda_%s_R_%s_envelope.mat' %(str(epoch + 1), str(lamb), str(R)),\n","                            {'T_PINN': T_PINN, 'X_PINN': X_PINN, 'Y_PINN': Y_PINN, 'U_PINN': U_PINN, 'V_PINN': V_PINN, 'P_PINN': P_PINN,\n","                            'T_WS': T_WS, 'X_WS': X_WS, 'Y_WS': Y_WS, 'U_WS': U_WS, 'V_WS': V_WS, 'P_WS': P_WS,\n","                            'U_WS_pred': U_WS_pred, 'V_WS_pred': V_WS_pred, 'P_WS_pred': P_WS_pred,\n","                            'T_val': T_val, 'X_val': X_val, 'Y_val': Y_val, 'U_val': U_val, 'V_val': V_val, 'P_val': P_val,\n","                            'U_val_pred': U_val_pred, 'V_val_pred': V_val_pred, 'P_valt_pred': P_val_pred,\n","                            'Train_loss' : train_loss_results, 'NS_loss' : NS_loss_results,\n","                            'P_loss' : P_loss_results, 'U_loss' : U_loss_results, 'V_loss' : V_loss_results}) # Change ending of the .mat name according to validation case selected: close, far or envelope\n","\n","print('Process completed')"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"VtWQ8dSzqFZ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747613768647,"user_tz":300,"elapsed":18,"user":{"displayName":"sebastian alejandro gomez ardila","userId":"12249047818400838393"}},"outputId":"70f3435a-55f8-4658-d98b-0dd08e1bf557"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":38}],"source":["1+1"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsh5vMQ5qRzzxN8h/UOXxM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}